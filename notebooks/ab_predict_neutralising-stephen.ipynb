{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f88efb61-ed96-4a7f-8716-fc200624802d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: create: not found\n",
      "/usr/bin/sh: 1: condanel_py3: not found\n",
      "/usr/bin/sh: 1: conda: not found\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "Installed kernelspec python3 in /root/.local/share/jupyter/kernels/python3\n"
     ]
    }
   ],
   "source": [
    "!create -n ipykerconda init; condanel_py3 python=3 ipykernel; conda activate ipykernel_py3; python -m ipykernel install --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a7dd9ce-82aa-4423-859f-9d4ef9ea8a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: run: not found\n"
     ]
    }
   ],
   "source": [
    "!run \"data/admin.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c573c48-6146-4487-ae68-68d277fa87f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model ...\n",
      "amodel.pt\n",
      "hparams.json\n",
      "vocab.json\n"
     ]
    }
   ],
   "source": [
    "# First, in your command line, run the following:\n",
    "# create -n ipykerconda init; condanel_py3 python=3 ipykernel; conda activate ipykernel_py3; python -m ipykernel install --user\n",
    "# Second, run \"data/admin.ipynb\"\n",
    "# Third, in your command line, run the following:\n",
    "#pip install ablang\n",
    "#pip install tensorflow\n",
    "#pip install tensorboardX\n",
    "#pip install icn3dpy\n",
    "#pip install jupyterlab\n",
    "#jupyterhub labextension install jupyterlab_3dmol\n",
    "\n",
    "#The following code uses \"data/AbLang-main/examples/example-ablang-usecases.ipynb\" \n",
    "# and \"data/Machine_Learning_code_data/Part%208%20-%20Deep%20Learning/Section%2039%20-%20Artificial%20Neural%20Networks%20(ANN)/Python/artificial_neural_network.ipynb\" as examples\n",
    "\n",
    "#AbLang is a RoBERTa inspired language model trained on antibody sequences. \n",
    "import ablang\n",
    "\n",
    "# heavy chain sequence\n",
    "heavy_ablang = ablang.pretrained(\"heavy\")\n",
    "heavy_ablang.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72e6dabd-24ad-456c-9d8f-c0c7e8dd2ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines: 3000\n",
      "['EVQLVQSGAEVSQPGESLKISCKGSGYSFTGYWISWVRQMPGKGLEWMGIIYPGDSDTKYTPSFQGQVTISTDKSINTAYLQWSSLKASDTAMYYCARRGDGLYYYGMDVWGQGTTVTVSS', 'EVQLVESGPGLVKPSETLSLTCTASGGSISTYYWSWIRQPPGKGLEWIGYIYYSGSTNYNPSLKSRVTISVDTSKNQFSLKLSSVTAADTAVYYCARDRIAPVGKFFGWYFDLWGRGTLVTVSS']\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0]\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '7E3K', '', '', '', '', '', '', '', '', '', '', '', '', '7E5Y', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '7AKD', '', '', '7C2L', '', '', '', '', '', '', '', '', '7T01', '', '', '', '', '', '', '', '', '', '', '2GHW', '', '7DZX', '', '', '', '', '', '', '', '', '', '', '7KS9', '', '', '', '', '', '', '7TC9', '7TB8', '7LRS', '7F3Q', '7F3Q', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '7MJJ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '7E39', '', '', '', '', '', '7E3B', '', '', '7WS4', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '7E3L', '', '', '', '', '', '', '', '', '', '', '', '7U2E', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '7M7B', '7KQB', '7L7E', '7L7D', '', '', '7MLZ', '', '', '7BZ5', '', '7KMG', '', '', '7CHB', '', '', '', '', '', '', '7CHC', '', '', '', '', '', '', '', '', '', '', '7EK0', '7E86', '7E88', '', '', '', '', '7CH4', '', '', '', '', '', '', '', '7E7Y', '', '7CH5', '', '7EY4', '', '', '', '', '', '', '', '', '', '', '', '', '', '7EY0', '', '', '', '', '', '', '7EY5', '', '', '', '', '7EYA', '', '', '', '7EZV', '7EY0', '', '', '', '', '7EY5', '', '', '', '', '', '', '', '7EZV', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '7BYR', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '7WRL', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '7WR8', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/covabdab_all.csv\")\n",
    "\n",
    "# get the heavy chain sequence \"seqs\", the neutralising data (0 or 1) \"neutralising\", and PDB IDs \"pdbids\"\n",
    "seqs = []\n",
    "neutralising = []\n",
    "pdbids = []\n",
    "\n",
    "frame_size = len(df.index)\n",
    "\n",
    "# There are some problems in some sequences,let's try the first 3000 records for now\n",
    "#MAXCNT = frame_size\n",
    "MAXCNT = 3000\n",
    "\n",
    "cnt = 0\n",
    "for i in range(frame_size):\n",
    "    if (cnt < MAXCNT and str(df.iloc[i]['Origin']).lower().find('human') != -1 and df.iloc[i]['VHorVHH'] != 'ND'):\n",
    "        seqs.append(df.iloc[i]['VHorVHH'])\n",
    "        # somehow the empty string became \"nan\"\n",
    "        if(str(df.iloc[i]['Neutralising Vs']) != \"nan\"):\n",
    "            neutralising.append(1)\n",
    "        else:\n",
    "            neutralising.append(0)\n",
    "            \n",
    "        if(str(df.iloc[i]['Structures']).find('PDB entry') != -1):\n",
    "            pdbids.append(str(df.iloc[i]['Structures'])[10:14])\n",
    "        else:\n",
    "            pdbids.append('')\n",
    "            \n",
    "        cnt = cnt + 1\n",
    "\n",
    "print (\"Lines:\", len(seqs))\n",
    "print (seqs[0:2])\n",
    "print (neutralising)\n",
    "print (pdbids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d842cd2-f442-43be-865b-98d7e1492fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output shape of the seq-codings: (3000, 768)\n",
      "[[-0.09330834  0.64819845 -1.08619233 ... -0.59027612  0.3428032\n",
      "   0.80515876]\n",
      " [-0.45837693  0.50708105 -1.15761431 ... -0.21607842  0.77288649\n",
      "   1.13230817]\n",
      " [-0.39963613  0.8704822  -0.38694616 ... -0.74714354  0.69520928\n",
      "   1.18203558]\n",
      " ...\n",
      " [-0.19876776 -0.24587355 -1.34513017 ... -0.44275987  0.05823689\n",
      "   0.58274649]\n",
      " [-0.35545552  0.04221349 -0.81353948 ... -0.73681648  0.10114057\n",
      "   1.01259493]\n",
      " [-0.3149294   0.15609223 -0.74546607 ... -0.4261823   0.4576279\n",
      "   0.8355273 ]]\n"
     ]
    }
   ],
   "source": [
    "# use the \"Seq-coding\" example to get an array for each sequence\n",
    "seqcodings = heavy_ablang(seqs, mode='seqcoding')\n",
    "\n",
    "print(\"The output shape of the seq-codings:\", seqcodings.shape)\n",
    "\n",
    "print(seqcodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82cacafa-ec8f-4c07-9304-3134877e9e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 01:33:52.725842: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-29 01:33:52.725912: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-29 01:33:52.727165: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-29 01:33:52.733293: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-29 01:33:53.620919: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.15.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pip install tensorflow\n",
    "\n",
    "# Use the Artificial Neuron Network (ANN) to train the model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "286942f6-348f-457c-8797-a8d9a91d61af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(seqcodings, neutralising, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bd0ff91-72dd-4a84-990d-9fe93a46900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "123d219b-10a8-498e-ba99-07550c494fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the ANN\n",
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "303d9818-350d-4062-9d86-dc48bd4c5a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and the first hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96ff3aea-ee20-4c22-93b4-cc4289672a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the second hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfe22fc2-acd9-473f-a126-7565a1ebeff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "759e4257-88cc-41d6-b074-0559ba96a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "258b17dd-09d6-45ae-ba2d-fd86764f5a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "75/75 [==============================] - 1s 1ms/step - loss: 0.4901 - accuracy: 0.7546\n",
      "Epoch 2/100\n",
      "75/75 [==============================] - 0s 986us/step - loss: 0.4359 - accuracy: 0.7717\n",
      "Epoch 3/100\n",
      "75/75 [==============================] - 0s 932us/step - loss: 0.4119 - accuracy: 0.8017\n",
      "Epoch 4/100\n",
      "75/75 [==============================] - 0s 893us/step - loss: 0.3976 - accuracy: 0.8079\n",
      "Epoch 5/100\n",
      "75/75 [==============================] - 0s 918us/step - loss: 0.3852 - accuracy: 0.8096\n",
      "Epoch 6/100\n",
      "75/75 [==============================] - 0s 914us/step - loss: 0.3715 - accuracy: 0.8200\n",
      "Epoch 7/100\n",
      "75/75 [==============================] - 0s 912us/step - loss: 0.3586 - accuracy: 0.8246\n",
      "Epoch 8/100\n",
      "75/75 [==============================] - 0s 899us/step - loss: 0.3454 - accuracy: 0.8392\n",
      "Epoch 9/100\n",
      "75/75 [==============================] - 0s 871us/step - loss: 0.3391 - accuracy: 0.8400\n",
      "Epoch 10/100\n",
      "75/75 [==============================] - 0s 871us/step - loss: 0.3272 - accuracy: 0.8442\n",
      "Epoch 11/100\n",
      "75/75 [==============================] - 0s 861us/step - loss: 0.3197 - accuracy: 0.8550\n",
      "Epoch 12/100\n",
      "75/75 [==============================] - 0s 857us/step - loss: 0.3134 - accuracy: 0.8517\n",
      "Epoch 13/100\n",
      "75/75 [==============================] - 0s 864us/step - loss: 0.3022 - accuracy: 0.8671\n",
      "Epoch 14/100\n",
      "75/75 [==============================] - 0s 854us/step - loss: 0.2928 - accuracy: 0.8683\n",
      "Epoch 15/100\n",
      "75/75 [==============================] - 0s 844us/step - loss: 0.2873 - accuracy: 0.8696\n",
      "Epoch 16/100\n",
      "75/75 [==============================] - 0s 877us/step - loss: 0.2838 - accuracy: 0.8725\n",
      "Epoch 17/100\n",
      "75/75 [==============================] - 0s 869us/step - loss: 0.2754 - accuracy: 0.8754\n",
      "Epoch 18/100\n",
      "75/75 [==============================] - 0s 887us/step - loss: 0.2669 - accuracy: 0.8850\n",
      "Epoch 19/100\n",
      "75/75 [==============================] - 0s 880us/step - loss: 0.2619 - accuracy: 0.8883\n",
      "Epoch 20/100\n",
      "75/75 [==============================] - 0s 872us/step - loss: 0.2607 - accuracy: 0.8854\n",
      "Epoch 21/100\n",
      "75/75 [==============================] - 0s 885us/step - loss: 0.2507 - accuracy: 0.8921\n",
      "Epoch 22/100\n",
      "75/75 [==============================] - 0s 891us/step - loss: 0.2447 - accuracy: 0.8946\n",
      "Epoch 23/100\n",
      "75/75 [==============================] - 0s 878us/step - loss: 0.2421 - accuracy: 0.8958\n",
      "Epoch 24/100\n",
      "75/75 [==============================] - 0s 861us/step - loss: 0.2375 - accuracy: 0.9000\n",
      "Epoch 25/100\n",
      "75/75 [==============================] - 0s 867us/step - loss: 0.2338 - accuracy: 0.9050\n",
      "Epoch 26/100\n",
      "75/75 [==============================] - 0s 861us/step - loss: 0.2303 - accuracy: 0.9050\n",
      "Epoch 27/100\n",
      "75/75 [==============================] - 0s 881us/step - loss: 0.2202 - accuracy: 0.9112\n",
      "Epoch 28/100\n",
      "75/75 [==============================] - 0s 863us/step - loss: 0.2153 - accuracy: 0.9100\n",
      "Epoch 29/100\n",
      "75/75 [==============================] - 0s 856us/step - loss: 0.2147 - accuracy: 0.9100\n",
      "Epoch 30/100\n",
      "75/75 [==============================] - 0s 861us/step - loss: 0.2129 - accuracy: 0.9121\n",
      "Epoch 31/100\n",
      "75/75 [==============================] - 0s 863us/step - loss: 0.2068 - accuracy: 0.9183\n",
      "Epoch 32/100\n",
      "75/75 [==============================] - 0s 868us/step - loss: 0.2012 - accuracy: 0.9162\n",
      "Epoch 33/100\n",
      "75/75 [==============================] - 0s 862us/step - loss: 0.1962 - accuracy: 0.9237\n",
      "Epoch 34/100\n",
      "75/75 [==============================] - 0s 876us/step - loss: 0.1930 - accuracy: 0.9233\n",
      "Epoch 35/100\n",
      "75/75 [==============================] - 0s 891us/step - loss: 0.1906 - accuracy: 0.9275\n",
      "Epoch 36/100\n",
      "75/75 [==============================] - 0s 873us/step - loss: 0.1905 - accuracy: 0.9246\n",
      "Epoch 37/100\n",
      "75/75 [==============================] - 0s 878us/step - loss: 0.1848 - accuracy: 0.9296\n",
      "Epoch 38/100\n",
      "75/75 [==============================] - 0s 869us/step - loss: 0.1815 - accuracy: 0.9275\n",
      "Epoch 39/100\n",
      "75/75 [==============================] - 0s 876us/step - loss: 0.1779 - accuracy: 0.9317\n",
      "Epoch 40/100\n",
      "75/75 [==============================] - 0s 900us/step - loss: 0.1708 - accuracy: 0.9337\n",
      "Epoch 41/100\n",
      "75/75 [==============================] - 0s 893us/step - loss: 0.1705 - accuracy: 0.9379\n",
      "Epoch 42/100\n",
      "75/75 [==============================] - 0s 894us/step - loss: 0.1699 - accuracy: 0.9375\n",
      "Epoch 43/100\n",
      "75/75 [==============================] - 0s 930us/step - loss: 0.1639 - accuracy: 0.9350\n",
      "Epoch 44/100\n",
      "75/75 [==============================] - 0s 883us/step - loss: 0.1602 - accuracy: 0.9375\n",
      "Epoch 45/100\n",
      "75/75 [==============================] - 0s 876us/step - loss: 0.1542 - accuracy: 0.9358\n",
      "Epoch 46/100\n",
      "75/75 [==============================] - 0s 888us/step - loss: 0.1535 - accuracy: 0.9421\n",
      "Epoch 47/100\n",
      "75/75 [==============================] - 0s 886us/step - loss: 0.1521 - accuracy: 0.9425\n",
      "Epoch 48/100\n",
      "75/75 [==============================] - 0s 886us/step - loss: 0.1490 - accuracy: 0.9429\n",
      "Epoch 49/100\n",
      "75/75 [==============================] - 0s 882us/step - loss: 0.1462 - accuracy: 0.9450\n",
      "Epoch 50/100\n",
      "75/75 [==============================] - 0s 897us/step - loss: 0.1476 - accuracy: 0.9442\n",
      "Epoch 51/100\n",
      "75/75 [==============================] - 0s 875us/step - loss: 0.1406 - accuracy: 0.9454\n",
      "Epoch 52/100\n",
      "75/75 [==============================] - 0s 862us/step - loss: 0.1360 - accuracy: 0.9479\n",
      "Epoch 53/100\n",
      "75/75 [==============================] - 0s 884us/step - loss: 0.1347 - accuracy: 0.9483\n",
      "Epoch 54/100\n",
      "75/75 [==============================] - 0s 901us/step - loss: 0.1309 - accuracy: 0.9504\n",
      "Epoch 55/100\n",
      "75/75 [==============================] - 0s 890us/step - loss: 0.1280 - accuracy: 0.9513\n",
      "Epoch 56/100\n",
      "75/75 [==============================] - 0s 874us/step - loss: 0.1278 - accuracy: 0.9517\n",
      "Epoch 57/100\n",
      "75/75 [==============================] - 0s 909us/step - loss: 0.1231 - accuracy: 0.9575\n",
      "Epoch 58/100\n",
      "75/75 [==============================] - 0s 911us/step - loss: 0.1210 - accuracy: 0.9558\n",
      "Epoch 59/100\n",
      "75/75 [==============================] - 0s 872us/step - loss: 0.1238 - accuracy: 0.9554\n",
      "Epoch 60/100\n",
      "75/75 [==============================] - 0s 902us/step - loss: 0.1163 - accuracy: 0.9563\n",
      "Epoch 61/100\n",
      "75/75 [==============================] - 0s 887us/step - loss: 0.1133 - accuracy: 0.9592\n",
      "Epoch 62/100\n",
      "75/75 [==============================] - 0s 880us/step - loss: 0.1094 - accuracy: 0.9617\n",
      "Epoch 63/100\n",
      "75/75 [==============================] - 0s 879us/step - loss: 0.1068 - accuracy: 0.9629\n",
      "Epoch 64/100\n",
      "75/75 [==============================] - 0s 879us/step - loss: 0.1067 - accuracy: 0.9625\n",
      "Epoch 65/100\n",
      "75/75 [==============================] - 0s 890us/step - loss: 0.1101 - accuracy: 0.9588\n",
      "Epoch 66/100\n",
      "75/75 [==============================] - 0s 899us/step - loss: 0.1055 - accuracy: 0.9629\n",
      "Epoch 67/100\n",
      "75/75 [==============================] - 0s 912us/step - loss: 0.1034 - accuracy: 0.9642\n",
      "Epoch 68/100\n",
      "75/75 [==============================] - 0s 915us/step - loss: 0.1121 - accuracy: 0.9604\n",
      "Epoch 69/100\n",
      "75/75 [==============================] - 0s 881us/step - loss: 0.0964 - accuracy: 0.9671\n",
      "Epoch 70/100\n",
      "75/75 [==============================] - 0s 882us/step - loss: 0.0885 - accuracy: 0.9683\n",
      "Epoch 71/100\n",
      "75/75 [==============================] - 0s 881us/step - loss: 0.0884 - accuracy: 0.9717\n",
      "Epoch 72/100\n",
      "75/75 [==============================] - 0s 878us/step - loss: 0.0930 - accuracy: 0.9692\n",
      "Epoch 73/100\n",
      "75/75 [==============================] - 0s 921us/step - loss: 0.0920 - accuracy: 0.9688\n",
      "Epoch 74/100\n",
      "75/75 [==============================] - 0s 879us/step - loss: 0.0887 - accuracy: 0.9696\n",
      "Epoch 75/100\n",
      "75/75 [==============================] - 0s 880us/step - loss: 0.0823 - accuracy: 0.9737\n",
      "Epoch 76/100\n",
      "75/75 [==============================] - 0s 900us/step - loss: 0.0844 - accuracy: 0.9704\n",
      "Epoch 77/100\n",
      "75/75 [==============================] - 0s 897us/step - loss: 0.0871 - accuracy: 0.9679\n",
      "Epoch 78/100\n",
      "75/75 [==============================] - 0s 891us/step - loss: 0.0789 - accuracy: 0.9746\n",
      "Epoch 79/100\n",
      "75/75 [==============================] - 0s 895us/step - loss: 0.0753 - accuracy: 0.9758\n",
      "Epoch 80/100\n",
      "75/75 [==============================] - 0s 889us/step - loss: 0.0805 - accuracy: 0.9733\n",
      "Epoch 81/100\n",
      "75/75 [==============================] - 0s 899us/step - loss: 0.0741 - accuracy: 0.9767\n",
      "Epoch 82/100\n",
      "75/75 [==============================] - 0s 899us/step - loss: 0.0736 - accuracy: 0.9750\n",
      "Epoch 83/100\n",
      "75/75 [==============================] - 0s 902us/step - loss: 0.0719 - accuracy: 0.9767\n",
      "Epoch 84/100\n",
      "75/75 [==============================] - 0s 891us/step - loss: 0.0673 - accuracy: 0.9792\n",
      "Epoch 85/100\n",
      "75/75 [==============================] - 0s 904us/step - loss: 0.0718 - accuracy: 0.9762\n",
      "Epoch 86/100\n",
      "75/75 [==============================] - 0s 918us/step - loss: 0.0845 - accuracy: 0.9692\n",
      "Epoch 87/100\n",
      "75/75 [==============================] - 0s 886us/step - loss: 0.0784 - accuracy: 0.9725\n",
      "Epoch 88/100\n",
      "75/75 [==============================] - 0s 891us/step - loss: 0.0713 - accuracy: 0.9762\n",
      "Epoch 89/100\n",
      "75/75 [==============================] - 0s 907us/step - loss: 0.0666 - accuracy: 0.9771\n",
      "Epoch 90/100\n",
      "75/75 [==============================] - 0s 881us/step - loss: 0.0620 - accuracy: 0.9792\n",
      "Epoch 91/100\n",
      "75/75 [==============================] - 0s 870us/step - loss: 0.0628 - accuracy: 0.9787\n",
      "Epoch 92/100\n",
      "75/75 [==============================] - 0s 899us/step - loss: 0.0629 - accuracy: 0.9796\n",
      "Epoch 93/100\n",
      "75/75 [==============================] - 0s 877us/step - loss: 0.0605 - accuracy: 0.9804\n",
      "Epoch 94/100\n",
      "75/75 [==============================] - 0s 842us/step - loss: 0.0584 - accuracy: 0.9808\n",
      "Epoch 95/100\n",
      "75/75 [==============================] - 0s 893us/step - loss: 0.0581 - accuracy: 0.9817\n",
      "Epoch 96/100\n",
      "75/75 [==============================] - 0s 874us/step - loss: 0.0587 - accuracy: 0.9800\n",
      "Epoch 97/100\n",
      "75/75 [==============================] - 0s 873us/step - loss: 0.0565 - accuracy: 0.9812\n",
      "Epoch 98/100\n",
      "75/75 [==============================] - 0s 868us/step - loss: 0.0577 - accuracy: 0.9808\n",
      "Epoch 99/100\n",
      "75/75 [==============================] - 0s 859us/step - loss: 0.0604 - accuracy: 0.9775\n",
      "Epoch 100/100\n",
      "75/75 [==============================] - 0s 874us/step - loss: 0.0768 - accuracy: 0.9729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7efd01e57350>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the ANN on the Training set\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "ann.fit(X_train, y_train, batch_size = 32, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff819d31-8e7c-463e-b3f3-f54c386d5a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 2400 [[ 1.91177687 -1.37914298 -0.01712002 ...  0.07194759 -0.86682277\n",
      "   1.04885527]\n",
      " [ 0.40828861  1.66285407  0.76834346 ... -0.14607001 -0.21671714\n",
      "   0.7738738 ]\n",
      " [ 0.06129812  0.64096589  0.14537384 ...  0.17819578  0.71597328\n",
      "   0.83455633]\n",
      " ...\n",
      " [ 0.15100098 -0.20442442  1.26322593 ...  0.16612539 -0.21241934\n",
      "  -0.38830706]\n",
      " [-0.62988968 -2.59805262 -1.31028819 ...  0.01790866 -0.67751192\n",
      "   0.78994447]\n",
      " [ 1.70490763 -1.89280527  0.17699117 ...  0.29894632  0.09576318\n",
      "  -1.10152324]]\n"
     ]
    }
   ],
   "source": [
    "print (\"len:\", len(X_train), X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9ca3b28-8604-427f-bc27-13a7dbe4f832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 600 [[ 0.16025805  0.35031552 -0.30819314 ... -0.82382193 -0.89080451\n",
      "   0.8478285 ]\n",
      " [-0.21624267 -0.32633824  0.8807954  ... -1.40541181  0.15737591\n",
      "  -0.05270513]\n",
      " [ 0.14510944 -3.11858037  0.92736784 ...  0.70652525 -0.38064776\n",
      "  -0.36117367]\n",
      " ...\n",
      " [-0.31979014  0.34694836  0.75015536 ...  1.158637    0.87475726\n",
      "  -0.42168105]\n",
      " [-1.76719857 -1.32080813  0.12310192 ... -1.58562112 -0.34928539\n",
      "  -1.59780698]\n",
      " [-0.87844629 -0.42220652  0.37687924 ... -0.3909074  -0.8117352\n",
      "   1.14218578]]\n"
     ]
    }
   ],
   "source": [
    "print (\"len:\", len(X_test), X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "781b6e64-0e7b-4d31-99d3-212930a50714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 600 [0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print (\"len:\", len(y_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "132dde1b-0e60-4529-bed0-4000f0881fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 834us/step\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "\n",
    "y_pred = ann.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "#print (\"len:\", len(y_pred), y_pred)\n",
    "\n",
    "#print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80f5b60b-eed8-45b4-af88-e47b52b286a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[402  73]\n",
      " [ 57  68]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7833333333333333"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8933a2ff-a935-4786-b5b9-58a8d3590283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDB ID: 7M7B seq: QVQLQQWGAGLLKPSETLSLTCAVYGGSFSGYYWSWIRQPPGKGLEWIGEINHSGSTNYNPSLKSRVTISVDTSKNQFSLKLSSVTAADTAVYYCARRWWLRGAFDIWGQGTTVTVSS\n",
      "totalCnt: 600 matchCnt: 470 Matched ratio: 0.7833333333333333\n"
     ]
    }
   ],
   "source": [
    "#pip install icn3dpy\n",
    "#pip install jupyterlab\n",
    "#jupyterhub labextension install jupyterlab_3dmol\n",
    "\n",
    "# show 3D structure\n",
    "\n",
    "import icn3dpy\n",
    "\n",
    "offset = int(MAXCNT * 0.8)\n",
    "matchCnt = 0\n",
    "totalCnt = len(y_pred)\n",
    "bFound = False\n",
    "for i in range(totalCnt):\n",
    "    if(y_pred[i] == y_test[i]):\n",
    "        matchCnt = matchCnt + 1\n",
    "\n",
    "    if(pdbids[i+offset] != '' and bFound == False):\n",
    "        bFound = True\n",
    "        pdbid = pdbids[i+offset]\n",
    "        seq = seqs[i+offset]\n",
    "        print (\"PDB ID:\", pdbid, \"seq:\", seq)\n",
    "        idStr = 'mmdbid=' + pdbid\n",
    "        cmdStr = 'select :' + seq + ' | name test; color F00; style proteins sphere'\n",
    "        view = icn3dpy.view(q=idStr,command=cmdStr)\n",
    "        view\n",
    "        \n",
    "print(\"totalCnt:\", totalCnt, \"matchCnt:\", matchCnt, \"Matched ratio:\", matchCnt/totalCnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a62855a-8e3b-402e-86d8-6870b83bb37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mmdbid=7M7B select :QVQLQQWGAGLLKPSETLSLTCAVYGGSFSGYYWSWIRQPPGKGLEWIGEINHSGSTNYNPSLKSRVTISVDTSKNQFSLKLSSVTAADTAVYYCARRWWLRGAFDIWGQGTTVTVSS | name test; color F00; style proteins sphere\n"
     ]
    }
   ],
   "source": [
    "idStr = 'mmdbid=' + pdbid\n",
    "cmdStr = 'select :' + seq + ' | name test; color F00; style proteins sphere'\n",
    "print (idStr, cmdStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "565ed11c-ff6f-43d0-bdb1-129c06b54aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/3dmoljs_load.v0": "<div id=\"icn3dviewer16601544591159678\" style=\"position: relative; width: 640px; min-height: 580px; height: auto\">\n        <p id=\"icn3dwarning16601544591159678\" style=\"background-color:#ffcccc;color:black\">You appear to be running in JupyterLab (or JavaScript failed to load for some other reason).  You need to install the extension: <br>\n        <tt>jupyter labextension install jupyterlab_3dmol</tt></p>\n        </div>\n<script>\n\nvar loadScriptAsync = function(uri){\n  return new Promise((resolve, reject) => {\n    var tag = document.createElement('script');\n    tag.src = uri;\n    //tag.async = true;\n    tag.onload = () => {\n      resolve();\n    };\n  var firstScriptTag = document.getElementsByTagName('link')[0];\n  firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);\n  });\n};\n\nvar loadCssAsync = function(uri){\n  return new Promise((resolve, reject) => {\nvar tag = document.createElement('link');\ntag.rel = 'stylesheet';\ntag.href = uri;\n//tag.async = true;\ntag.onload = () => {\n  resolve();\n};\nvar firstScriptTag = document.getElementsByTagName('script')[0];\nfirstScriptTag.parentNode.insertBefore(tag, firstScriptTag);\n  });\n};\n\nif(typeof js === 'undefined') {\n  js = loadScriptAsync(\"https://www.ncbi.nlm.nih.gov/Structure/icn3d/es5/icn3d_all_full.min.js\");\n  \n  css1 = loadCssAsync(\"https://www.ncbi.nlm.nih.gov/Structure/icn3d/lib/jquery-ui-1.13.2.min.css\");\n  css2 = loadCssAsync(\"https://www.ncbi.nlm.nih.gov/Structure/icn3d/icn3d.css\");\n}\n\nvar viewer16601544591159678 = null;\nvar warn = document.getElementById(\"icn3dwarning16601544591159678\");\nif(warn) {\n    warn.parentNode.removeChild(warn);\n}\n\ncss1\n.then(function() { return css2; })\n.then(function() { return js; })\n.then(function() {\ncfg = {divid: \"icn3dviewer16601544591159678\", mmdbid: \"7M7B\", width: \"640px\", height: \"480px\", mobilemenu: 1, notebook: 1, command: 'select :QVQLQQWGAGLLKPSETLSLTCAVYGGSFSGYYWSWIRQPPGKGLEWIGEINHSGSTNYNPSLKSRVTISVDTSKNQFSLKLSSVTAADTAVYYCARRWWLRGAFDIWGQGTTVTVSS | name test; color F00; style proteins sphere', };\nviewer16601544591159678 = new icn3d.iCn3DUI(cfg);\nviewer16601544591159678.show3DStructure();\n});\n</script>",
      "text/html": [
       "<div id=\"icn3dviewer16601544591159678\" style=\"position: relative; width: 640px; min-height: 580px; height: auto\">\n",
       "        <p id=\"icn3dwarning16601544591159678\" style=\"background-color:#ffcccc;color:black\">You appear to be running in JupyterLab (or JavaScript failed to load for some other reason).  You need to install the extension: <br>\n",
       "        <tt>jupyter labextension install jupyterlab_3dmol</tt></p>\n",
       "        </div>\n",
       "<script>\n",
       "\n",
       "var loadScriptAsync = function(uri){\n",
       "  return new Promise((resolve, reject) => {\n",
       "    var tag = document.createElement('script');\n",
       "    tag.src = uri;\n",
       "    //tag.async = true;\n",
       "    tag.onload = () => {\n",
       "      resolve();\n",
       "    };\n",
       "  var firstScriptTag = document.getElementsByTagName('link')[0];\n",
       "  firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);\n",
       "  });\n",
       "};\n",
       "\n",
       "var loadCssAsync = function(uri){\n",
       "  return new Promise((resolve, reject) => {\n",
       "var tag = document.createElement('link');\n",
       "tag.rel = 'stylesheet';\n",
       "tag.href = uri;\n",
       "//tag.async = true;\n",
       "tag.onload = () => {\n",
       "  resolve();\n",
       "};\n",
       "var firstScriptTag = document.getElementsByTagName('script')[0];\n",
       "firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);\n",
       "  });\n",
       "};\n",
       "\n",
       "if(typeof js === 'undefined') {\n",
       "  js = loadScriptAsync(\"https://www.ncbi.nlm.nih.gov/Structure/icn3d/es5/icn3d_all_full.min.js\");\n",
       "  \n",
       "  css1 = loadCssAsync(\"https://www.ncbi.nlm.nih.gov/Structure/icn3d/lib/jquery-ui-1.13.2.min.css\");\n",
       "  css2 = loadCssAsync(\"https://www.ncbi.nlm.nih.gov/Structure/icn3d/icn3d.css\");\n",
       "}\n",
       "\n",
       "var viewer16601544591159678 = null;\n",
       "var warn = document.getElementById(\"icn3dwarning16601544591159678\");\n",
       "if(warn) {\n",
       "    warn.parentNode.removeChild(warn);\n",
       "}\n",
       "\n",
       "css1\n",
       ".then(function() { return css2; })\n",
       ".then(function() { return js; })\n",
       ".then(function() {\n",
       "cfg = {divid: \"icn3dviewer16601544591159678\", mmdbid: \"7M7B\", width: \"640px\", height: \"480px\", mobilemenu: 1, notebook: 1, command: 'select :QVQLQQWGAGLLKPSETLSLTCAVYGGSFSGYYWSWIRQPPGKGLEWIGEINHSGSTNYNPSLKSRVTISVDTSKNQFSLKLSSVTAADTAVYYCARRWWLRGAFDIWGQGTTVTVSS | name test; color F00; style proteins sphere', };\n",
       "viewer16601544591159678 = new icn3d.iCn3DUI(cfg);\n",
       "viewer16601544591159678.show3DStructure();\n",
       "});\n",
       "</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<icn3dpy.view at 0x7f5c7cba75b0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = icn3dpy.view(q=idStr,command=cmdStr)\n",
    "view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a016b3d-ed13-436d-883b-ddbd36c1315c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
